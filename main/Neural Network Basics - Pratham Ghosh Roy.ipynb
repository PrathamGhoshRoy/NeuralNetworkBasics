{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-16T23:49:59.098834Z","iopub.execute_input":"2023-12-16T23:49:59.099322Z","iopub.status.idle":"2023-12-16T23:49:59.144806Z","shell.execute_reply.started":"2023-12-16T23:49:59.099280Z","shell.execute_reply":"2023-12-16T23:49:59.143364Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network Basics\n\nThis is my personal documentation of my journey of learning the basics of Neural Networks.\n\nThanks to: [sentdex](https://www.youtube.com/@sentdex) for his lessons in NNFS (Neural Networks from Scratch)","metadata":{}},{"cell_type":"markdown","source":"#### First, lets import his module for accessing his datasets","metadata":{}},{"cell_type":"code","source":"!pip install nnfs","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:49:59.147414Z","iopub.execute_input":"2023-12-16T23:49:59.147859Z","iopub.status.idle":"2023-12-16T23:50:19.093458Z","shell.execute_reply.started":"2023-12-16T23:49:59.147818Z","shell.execute_reply":"2023-12-16T23:50:19.091487Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting nnfs\n  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nnfs) (1.24.3)\nInstalling collected packages: nnfs\nSuccessfully installed nnfs-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import nnfs","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.096816Z","iopub.execute_input":"2023-12-16T23:50:19.097279Z","iopub.status.idle":"2023-12-16T23:50:19.110003Z","shell.execute_reply.started":"2023-12-16T23:50:19.097239Z","shell.execute_reply":"2023-12-16T23:50:19.108674Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"An example of a 2-D array of inputs:","metadata":{}},{"cell_type":"code","source":"X = [[1, 2, 3, 2.5],\n     [2.0, 5.0, -1.0, 2.0],\n     [-1.5, 2.7, 3.3, -0,8]]","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.111822Z","iopub.execute_input":"2023-12-16T23:50:19.113223Z","iopub.status.idle":"2023-12-16T23:50:19.124870Z","shell.execute_reply.started":"2023-12-16T23:50:19.113168Z","shell.execute_reply":"2023-12-16T23:50:19.123527Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network:\nOkay, so here is a picture of a Neural Network for context:\n\n![](https://victorzhou.com/media/nn-series/network.svg)","metadata":{}},{"cell_type":"markdown","source":"It consists of:\n\n* **Neurons:** Those circles are called neurons.\n    It consists of:\n    * **Baises:** Values added (+) to the output.\n    * **Activation Function:** A function basically deciphers a neuron's importance in making the output decision.\n    \n        \n* **Layers:**\n    * Input Layer: The first layer where the model recieves its inputs (in blue)\n    * Hidden Layers: The middle two(or however many) consisting of 6 neurons(in this example) each (in black)\n    * Output Layer: The last layer at the end, which is the output of the model (in green)\n    \n    \n* **Weights:** The various lines which you see that joins each neuron of a particular layer to its neighbouring layers are called weights. They are multiplied with the previous layer's output.    \n","metadata":{}},{"cell_type":"markdown","source":"### The Neural Network Equation:\n\n\n![](https://pub.mdpi-res.com/universe/universe-08-00120/article_deploy/html/images/universe-08-00120-g001-550.jpg?1645603658)\n\n","metadata":{}},{"cell_type":"markdown","source":"Very similar to the equation: y = mx + b\nWhere,\n* **y** = output\n* **m** = weights\n* **x** = inputs\n* **b** = bias\n\nWhat it basically does is that it takes in the **inputs** from the **previous neuron**, **multiply them** with the **weights**, or the **connections(lines)**, and once it **reaches the current layer's neuron**, if and when it goes through the **activation function**, the **neuron's bias** gets added and then it **outputs** it to **the next layer**.","metadata":{}},{"cell_type":"markdown","source":"### Lets create our code for a Layer\n\n**Layer Dense**\n\nFirst, we are defining the weights:\n* **Weights** will be an array of **random numbers** of the dimension: **(number of inputs, n_neurons)** since **we need weights of each and every input per neuron.**\n\nWe will also **normalize** the weights by **multiplying it with a number(n) i.e. 1>n>0**, here lets take 0.10\n> self.weight = 0.10 * np.random.rand(n_inputs, n_neurons)\n\nNext, we will define the biases:\n\n* **Biases** will be (for now), **an array of zeroes** for **each neuron**, so the **dimension** will be: **(1, number of neurons)**\n\nWhy? \nWell, if the number of the biases are too big, it **might just explode** by the **time it comes to the output layer**. We **don't** want very big numbers because **they might explode and become incomputable**, hence we will keep the biases 0 for now.\n> self.biases = np.zeroes((1, n_neurons))\n\n**Drawback of having 0 biases:**\n\nIf **incase** the **sum of an equation for a neuron becomes 0** because of our bias (i.e. the value being added in the equation) is zero, the **input for the next layer will be 0**, and so will be its output, since anything multiplied by a 0 is 0. \n\n**We do not want** that **since that will create a ripple effect of outputs of 0 for each layer** and we will be left with something called a **dead network**.\n\nSo, if we have a **dead network**, try changing the biases to a **non-zero number**.","metadata":{}},{"cell_type":"markdown","source":"##### **Forward method of Layer Dense:**\n\nIt will simply output the dotproduct i.e. (y = mx) + b for each neuron, hence:\n> self.output = np.dot(inputs, self.weight) + self.biases","metadata":{}},{"cell_type":"code","source":"class Layer_Dense:\n    def __init__(self, n_inputs, n_neurons):\n        self.weight = 0.10 * np.random.rand(n_inputs, n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n        \n    def forward(self, inputs):\n        self.output = np.dot(inputs, self.weight) + self.biases      ","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.129105Z","iopub.execute_input":"2023-12-16T23:50:19.131928Z","iopub.status.idle":"2023-12-16T23:50:19.141091Z","shell.execute_reply.started":"2023-12-16T23:50:19.131856Z","shell.execute_reply":"2023-12-16T23:50:19.139892Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Activation Function\n\nEach neuron **should** consist of an activation function.\n\n**Activation Function:** A function which basically **decides whether** a **neuron should be activated or not** based on its **weighted sum of inputs**. The **choice of activation function** affects the network's ability to learn **complex patters** and **relationships** in data.\n\nFor our example, we will be using **ReLU (Rectified Linear Unit)** Function.","metadata":{}},{"cell_type":"markdown","source":"### Rectified Linear Unit (ReLU) function\n\n![](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24d1ac2cc1ded69730feb_relu.jpg)","metadata":{}},{"cell_type":"markdown","source":"**ReLU function:**\n\n> f(x) = max(0, x)\n\nIt basically **assings a value** of **0 (unactivated)** or **non-zero 'x' (activated)** to the **weighted sum of a neuron's inputs**.","metadata":{}},{"cell_type":"code","source":"class Activation_ReLU:\n    def forward(self, inputs):\n        self.output = np.maximum(0, inputs)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.142910Z","iopub.execute_input":"2023-12-16T23:50:19.144516Z","iopub.status.idle":"2023-12-16T23:50:19.162083Z","shell.execute_reply.started":"2023-12-16T23:50:19.144458Z","shell.execute_reply":"2023-12-16T23:50:19.160438Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Next we will make an **Softmax Activation** for our output layer","metadata":{}},{"cell_type":"markdown","source":"## Softmax Activation\n\n![](https://miro.medium.com/v2/resize:fit:1232/0*GxuMPOpGsMoN5RwI)","metadata":{}},{"cell_type":"markdown","source":"Softmax Activation function: \n\n![](https://docs-assets.developer.apple.com/published/c2185dfdcf/0ab139bc-3ff6-49d2-8b36-dcc98ef31102.png)","metadata":{}},{"cell_type":"markdown","source":"## Softmax vs Sigmoid\n\nSoftmax curve might be looking like the sigmoid curve, but there is quite a bit of difference: \n\n![](https://i.stack.imgur.com/iJ6vX.png)","metadata":{}},{"cell_type":"markdown","source":"## What is the softmax function doing?\n\nBasically, it is putting the output values (output of the model) through:\n* The exponential function i.e. -> y =e^x\n* Then normalizing it","metadata":{}},{"cell_type":"markdown","source":"**Remember:** The **inputs** here are the **outputs of our model/output layer**, hence it will be in **batch form** (since we will have a whole dataset of inputs for any given model, in most cases).\n\n","metadata":{}},{"cell_type":"markdown","source":"* First we will exponentiate each value subtracted by the max value in that batch.\n\n> y = e^(x - max(x))\n\nSo:\n\n> exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)\n\n#### Why are we doing that?\n\n\nWhat subtracting each value with that array's maximum value will basically do is:\n* Turn the **highest number** in that array into a **0**\n* **Rest of the numbers** will be **less than 0**\n\nThen when we exponentiate it:\n* The **max number, previously 0**, will **become 1** since **e^0 = 1.0**\n* All the **other numbers** in the array will be a **number less than 1** but **more than 0** (classification heaven: 0-1)\n* All other numbers(n) in the array will be: **1>n>0**","metadata":{}},{"cell_type":"markdown","source":"In the following code: \n> exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)\n\n**Remember, since:**\n\n* **we want the max of each respective array**, and **not the entire array**, we **do axis=1**.\n* And **keepdims=True** to **output the values** in the **same dimensions as the input**.","metadata":{}},{"cell_type":"markdown","source":"### Next, in the softmax function:\n\n* **Dividing the exponential values** by the **sum of the exponential values of respective arrays**\n\n**Why?**\n\nSo that we have a **normalized set of values for our outputs**, which will **help us achieve** our **goal of creating a neural network** which **works well with a multiple-classification problem**.\n","metadata":{}},{"cell_type":"code","source":"class Activation_Softmax:\n    def forward(self, inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.164151Z","iopub.execute_input":"2023-12-16T23:50:19.165029Z","iopub.status.idle":"2023-12-16T23:50:19.181326Z","shell.execute_reply.started":"2023-12-16T23:50:19.164983Z","shell.execute_reply":"2023-12-16T23:50:19.180002Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Now, will **input a dataset** which basically is **visually spiral** when **scatter plotted**.\n\nSomething like:\n\n![](https://telesens.co/wp-content/uploads/2017/09/img_59cbd07be1178.png)\n\nLink to spiral dataset code: https://github.com/Sentdex/nnfs/blob/master/nnfs/datasets/spiral.py","metadata":{}},{"cell_type":"markdown","source":"### Training Spiral Data:\n\nLets create training data in a **spiral format!**","metadata":{}},{"cell_type":"code","source":"from nnfs.datasets import spiral_data\n\nX, y = spiral_data(samples=100, classes=3)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.183494Z","iopub.execute_input":"2023-12-16T23:50:19.184324Z","iopub.status.idle":"2023-12-16T23:50:19.202724Z","shell.execute_reply.started":"2023-12-16T23:50:19.184269Z","shell.execute_reply":"2023-12-16T23:50:19.201253Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### For starters, lets create a **basic input layer**:","metadata":{}},{"cell_type":"code","source":"basic_dense1 = Layer_Dense(2,3)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.205436Z","iopub.execute_input":"2023-12-16T23:50:19.206761Z","iopub.status.idle":"2023-12-16T23:50:19.216330Z","shell.execute_reply.started":"2023-12-16T23:50:19.206684Z","shell.execute_reply":"2023-12-16T23:50:19.214985Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(basic_dense1)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.218466Z","iopub.execute_input":"2023-12-16T23:50:19.219596Z","iopub.status.idle":"2023-12-16T23:50:19.232126Z","shell.execute_reply.started":"2023-12-16T23:50:19.219512Z","shell.execute_reply":"2023-12-16T23:50:19.230681Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<__main__.Layer_Dense object at 0x78760d174df0>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Now, we create an **activation function** for basic_dense1","metadata":{}},{"cell_type":"code","source":"basic_activation1 = Activation_ReLU()","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.234423Z","iopub.execute_input":"2023-12-16T23:50:19.234918Z","iopub.status.idle":"2023-12-16T23:50:19.251842Z","shell.execute_reply.started":"2023-12-16T23:50:19.234876Z","shell.execute_reply":"2023-12-16T23:50:19.250490Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### Now, lets create the output layer:\n\nWith **softmax activation**.","metadata":{}},{"cell_type":"code","source":"basic_dense2 = Layer_Dense(3, 3)\nbasic_activation2 = Activation_Softmax()","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.254359Z","iopub.execute_input":"2023-12-16T23:50:19.255232Z","iopub.status.idle":"2023-12-16T23:50:19.265514Z","shell.execute_reply.started":"2023-12-16T23:50:19.255182Z","shell.execute_reply":"2023-12-16T23:50:19.263989Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Lets test it out!","metadata":{}},{"cell_type":"code","source":"basic_dense1.forward(X)\nbasic_activation1.forward(basic_dense1.output)\n\nbasic_dense2.forward(basic_activation1.output)\nbasic_activation2.forward(basic_dense2.output)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.267592Z","iopub.execute_input":"2023-12-16T23:50:19.268283Z","iopub.status.idle":"2023-12-16T23:50:19.287185Z","shell.execute_reply.started":"2023-12-16T23:50:19.268243Z","shell.execute_reply":"2023-12-16T23:50:19.285668Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(basic_activation2.output[:5])","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.292650Z","iopub.execute_input":"2023-12-16T23:50:19.293781Z","iopub.status.idle":"2023-12-16T23:50:19.300623Z","shell.execute_reply.started":"2023-12-16T23:50:19.293733Z","shell.execute_reply":"2023-12-16T23:50:19.299245Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[[0.33333333 0.33333333 0.33333333]\n [0.3333352  0.333324   0.33334079]\n [0.33334128 0.33331956 0.33333916]\n [0.33334248 0.33330637 0.33335114]\n [0.33334924 0.33330584 0.33334492]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Everything is working great!\n\nThat was our first ever neural network! Congrats!\n\n3.12.2023","metadata":{}},{"cell_type":"markdown","source":"# **Calculating Loss**","metadata":{}},{"cell_type":"markdown","source":"## **Categorical Cross-Entropy Loss**\n\nHere is the function in mathematical notation:","metadata":{}},{"cell_type":"markdown","source":"![](https://androidkt.com/wp-content/uploads/2023/05/Selection_098.png)","metadata":{}},{"cell_type":"markdown","source":"#### **Steps for calculating Categorical Cross-Entropy Loss (also called log-loss):**\n\nFor examples sake lets say the following is the output of our output layer after going through the softmax function:","metadata":{}},{"cell_type":"code","source":"softmax_output_eg = [0.7, 0.2, 0.1]","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.302164Z","iopub.execute_input":"2023-12-16T23:50:19.302612Z","iopub.status.idle":"2023-12-16T23:50:19.315482Z","shell.execute_reply.started":"2023-12-16T23:50:19.302575Z","shell.execute_reply":"2023-12-16T23:50:19.314376Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"And the following is the target output for the same:","metadata":{}},{"cell_type":"code","source":"target_output_eg = [1, 0, 0]","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.317117Z","iopub.execute_input":"2023-12-16T23:50:19.317783Z","iopub.status.idle":"2023-12-16T23:50:19.333176Z","shell.execute_reply.started":"2023-12-16T23:50:19.317746Z","shell.execute_reply":"2023-12-16T23:50:19.331365Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**The Categorical Cross-Entropy will calculate it as:** (For one-hot encoding outputs)\n\n> target value * log(predicted output)\n\nfor each indices. \n\nSum each indices and make the entire equation a negative (-).","metadata":{}},{"cell_type":"code","source":"import math\n\nloss_eg = -(math.log(softmax_output_eg[0])*target_output_eg[0] +\n            math.log(softmax_output_eg[1])*target_output_eg[1] +\n            math.log(softmax_output_eg[2])*target_output_eg[2])\n\nprint(f'Categorical Cross-Entropy Loss: {loss_eg}')","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.334702Z","iopub.execute_input":"2023-12-16T23:50:19.335791Z","iopub.status.idle":"2023-12-16T23:50:19.348608Z","shell.execute_reply.started":"2023-12-16T23:50:19.335749Z","shell.execute_reply":"2023-12-16T23:50:19.347449Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Categorical Cross-Entropy Loss: 0.35667494393873245\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Since the **product** of the **indices for which the target value is 0** will **result in a 0**, **the actual sum really sums down to:**","metadata":{}},{"cell_type":"code","source":"loss_eg = -(math.log(softmax_output_eg[0]))\n\nprint(f'Categorical Cross-Entropy Loss: {loss_eg}')","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.350224Z","iopub.execute_input":"2023-12-16T23:50:19.350656Z","iopub.status.idle":"2023-12-16T23:50:19.362899Z","shell.execute_reply.started":"2023-12-16T23:50:19.350619Z","shell.execute_reply":"2023-12-16T23:50:19.361653Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Categorical Cross-Entropy Loss: 0.35667494393873245\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We sucessfully calculated **Categorical Cross-Entropy Loss**, manually!","metadata":{}},{"cell_type":"markdown","source":"### Now, we know that we will be making predictions in batch, and \n\n","metadata":{}},{"cell_type":"markdown","source":"#### How to **implement loss** where the **output** is **sparse** **rather than one-hot encoded** when in batches:\n\nLets say our softmax outputs from the softmax activation function are in batches like so:","metadata":{}},{"cell_type":"code","source":"softmax_output_eg2 = [[0.7, 0.2, 0.1],\n                      [0.1, 0.5, 0.4],\n                      [0.02, 0.9, 0.08]]","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.365155Z","iopub.execute_input":"2023-12-16T23:50:19.366442Z","iopub.status.idle":"2023-12-16T23:50:19.375593Z","shell.execute_reply.started":"2023-12-16T23:50:19.366334Z","shell.execute_reply":"2023-12-16T23:50:19.374345Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"But, **our class targets are in a 1D vector** with the **following map**:\n\n* 0: dog\n* 1: cat\n* 2: human\n\nOur **sparse target vector** would be:","metadata":{}},{"cell_type":"code","source":"class_targets_eg2 = [0, 1, 1]","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.377528Z","iopub.execute_input":"2023-12-16T23:50:19.378598Z","iopub.status.idle":"2023-12-16T23:50:19.389184Z","shell.execute_reply.started":"2023-12-16T23:50:19.378531Z","shell.execute_reply":"2023-12-16T23:50:19.388062Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"So basically, in the case of sparse target vector, what the vector is basically saying is:\n\n**For the softmax output in index one**, it is **concerned with the 0th element of that list**, i.e. **in our case 0.7**\n\n**Second,** it is **concerned with the 1st element of that list**, i.e. **in our case: 0.5**\n\nSimilarly, **third is 0.9**","metadata":{}},{"cell_type":"code","source":"# Explaining correspondence. Not meant to be run!\n\nsoftmax_output_eg2 = [[0.7, 0.2, 0.1],                  class_targets_eg2 = [0, \n                     [0.1, 0.5, 0.4],                                        1, \n                     [0.02, 0.9, 0.08]]                                      1]","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.391211Z","iopub.execute_input":"2023-12-16T23:50:19.392726Z","iopub.status.idle":"2023-12-16T23:50:19.406701Z","shell.execute_reply.started":"2023-12-16T23:50:19.392681Z","shell.execute_reply":"2023-12-16T23:50:19.404659Z"},"trusted":true},"execution_count":21,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[21], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    softmax_output_eg2 = [[0.7, 0.2, 0.1],                  class_targets_eg2 = [0,\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"],"ename":"SyntaxError","evalue":"invalid syntax. Maybe you meant '==' or ':=' instead of '='? (239927523.py, line 3)","output_type":"error"}]},{"cell_type":"markdown","source":"The above code explains the correspondence. \n\nOne way we can get the values is by:","metadata":{}},{"cell_type":"code","source":"for targ_ids, distribution in zip(class_targets_eg2, softmax_output_eg2):\n    print(distribution[targ_ids])","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.408997Z","iopub.status.idle":"2023-12-16T23:50:19.410927Z","shell.execute_reply.started":"2023-12-16T23:50:19.410516Z","shell.execute_reply":"2023-12-16T23:50:19.410556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get exactly what we stated before, the values we are interested in for calculating loss.","metadata":{}},{"cell_type":"markdown","source":"#### **But there is a better and more efficient way to do it with NumPy:**\n\nFirst, just make turn our softmax outputs into a NumPy array\n\nThen:\n\n> print(softmax_output_eg2[[0, 1, 2], class_targets_eg2])\n\nWhat the above code is basically doing is a NumPy array feature where:\n\nWe are basically calling the values of our interested outputs by their indices.","metadata":{}},{"cell_type":"code","source":"softmax_output_eg2 = np.array([[0.7, 0.2, 0.1],\n                               [0.1, 0.5, 0.4],\n                               [0.02, 0.9, 0.08]])\n\nprint(softmax_output_eg2[[0, 1, 2], class_targets_eg2])","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.412663Z","iopub.status.idle":"2023-12-16T23:50:19.414189Z","shell.execute_reply.started":"2023-12-16T23:50:19.413827Z","shell.execute_reply":"2023-12-16T23:50:19.413864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Nice and clean!**","metadata":{}},{"cell_type":"markdown","source":"We can do one more thing for making it future proof.\n\nI.e. **rather than hard-coding the range of our softmax output, we can just call the range of it**:","metadata":{}},{"cell_type":"code","source":"print(softmax_output_eg2[range(len(softmax_output_eg2)), class_targets_eg2])","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.416211Z","iopub.status.idle":"2023-12-16T23:50:19.417187Z","shell.execute_reply.started":"2023-12-16T23:50:19.416860Z","shell.execute_reply":"2023-12-16T23:50:19.416893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now, **the final touch**. Since **Categorical Cross Entropy is the negative log of the target classes confidence**, we get:","metadata":{}},{"cell_type":"code","source":"\nneg_log_eg2 = -np.log(softmax_output_eg2[\n    range(len(softmax_output_eg2)), class_targets_eg2\n])\nprint(neg_log_eg2)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.419524Z","iopub.status.idle":"2023-12-16T23:50:19.420182Z","shell.execute_reply.started":"2023-12-16T23:50:19.419851Z","shell.execute_reply":"2023-12-16T23:50:19.419882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get our losses for each class!","metadata":{}},{"cell_type":"markdown","source":"Now, **to calculate the loss of the whole batch**, we just **mean it**.","metadata":{}},{"cell_type":"code","source":"average_loss_eg2 = np.mean(neg_log_eg2)\nprint(average_loss_eg2)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.422460Z","iopub.status.idle":"2023-12-16T23:50:19.423145Z","shell.execute_reply.started":"2023-12-16T23:50:19.422809Z","shell.execute_reply":"2023-12-16T23:50:19.422840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now, we might hit a problem with this, i.e. 0. Since -log(0) = infinity","metadata":{}},{"cell_type":"code","source":"print(-np.log(0))","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.425561Z","iopub.status.idle":"2023-12-16T23:50:19.426370Z","shell.execute_reply.started":"2023-12-16T23:50:19.426084Z","shell.execute_reply":"2023-12-16T23:50:19.426108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we **if try to mean it**, it **will throw an error** because it **obviously cannot take infinity in its calculation of finding the mean**.","metadata":{}},{"cell_type":"markdown","source":"A **simple fix** to that is to **clip the amount to a fairly insignificant amount**:\n\nLike:\n> -np.log(1e-7))\n\nAll the way to:\n> -np.log(1-1e-7))","metadata":{}},{"cell_type":"code","source":"print(-np.log(1e-7))\nprint(-np.log(1-1e-7))","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.427773Z","iopub.status.idle":"2023-12-16T23:50:19.428444Z","shell.execute_reply.started":"2023-12-16T23:50:19.428213Z","shell.execute_reply":"2023-12-16T23:50:19.428236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here is how we will clip them:**","metadata":{}},{"cell_type":"code","source":"y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:50:19.429749Z","iopub.status.idle":"2023-12-16T23:50:19.430461Z","shell.execute_reply.started":"2023-12-16T23:50:19.430214Z","shell.execute_reply":"2023-12-16T23:50:19.430235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Implementing the loss function into our Neural Network:**","metadata":{}},{"cell_type":"markdown","source":"**We will start by defining a common (parent) Loss class:**\n\nIts \"calculate\" function will take in:\n* **output** = predicted value outputs of the neural network\n* **y** = actual values of the batches\n\n\"**sample_losses**\" method might chance depending on the kind of loss metric.\n\n**data_loss** basically takes the sample of those losses and returns it.","metadata":{}},{"cell_type":"code","source":"class Loss:\n    def calculate(self, output, y):\n        sample_losses = self.forward(output, y)\n        data_loss = np.mean(sample_losses)\n        return data_loss","metadata":{"execution":{"iopub.status.busy":"2023-12-16T23:55:58.498515Z","iopub.execute_input":"2023-12-16T23:55:58.499858Z","iopub.status.idle":"2023-12-16T23:55:58.506446Z","shell.execute_reply.started":"2023-12-16T23:55:58.499804Z","shell.execute_reply":"2023-12-16T23:55:58.504901Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### **The Categorical Cross-Entropy Loss Class**\n\nIt will firstly **inherent** from the **class \"Loss\"**\n\nIt will have a **forward method** which will basically work on the **first line of the Loss class' \"calculate\" function**:\n> sample_losses = self.forward(output, y)\n\nThe forward method will take:\n* **y_pred** = Predicted y values of our neural network\n* **y_true** = Actual y values","metadata":{}},{"cell_type":"markdown","source":"First things first:\n\n* We will calculate the length of the predicted value vector.\n* Clip the predicted y value vector as discussed before.\n\nThen we need to take into account that the y_values might come out either in the form of **sparse target vector** or, **one-hot encoded 2D vectors**\n\n**Sparse Target Vector example:**\n> [1,0,1,0]\n\n**One-Hot Encoded Vector example:**\n> [[0,1], [1,0]]\n\nSo, **we want our loss function to be able to handle both of these cases** so we **write a query** to **understand the shape of the output vectors** and **deal with them accordingly**, as discussed above.\n\nThen, we just **-log(confidences)** and we **return our loss**.","metadata":{}},{"cell_type":"code","source":"class Loss_CategoricalCrossEntropy(Loss):\n    def forward(self, y_pred, y_true):\n        samples = len(y_pred)\n        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n        \n        if len(y_true.shape) == 1:\n            correct_confidences = y_pred_clipped[range(samples), y_true]\n            \n        elif len(y_true.shape) == 2:\n            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n            \n        negative_log_likelihoods = -np.log(correct_confidences)\n        return negative_log_likelihoods","metadata":{"execution":{"iopub.status.busy":"2023-12-17T00:09:33.315814Z","iopub.execute_input":"2023-12-17T00:09:33.316325Z","iopub.status.idle":"2023-12-17T00:09:33.325631Z","shell.execute_reply.started":"2023-12-17T00:09:33.316288Z","shell.execute_reply":"2023-12-17T00:09:33.323802Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Now, all that is left is for us to call our loss function on our softmax activation function outputs:","metadata":{}},{"cell_type":"code","source":"loss_function = Loss_CategoricalCrossEntropy()\nloss = loss_function.calculate(basic_activation2.output, y)\n\nprint(f'Loss: {loss}')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T00:10:58.174724Z","iopub.execute_input":"2023-12-17T00:10:58.175261Z","iopub.status.idle":"2023-12-17T00:10:58.182922Z","shell.execute_reply.started":"2023-12-17T00:10:58.175220Z","shell.execute_reply":"2023-12-17T00:10:58.181908Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Loss: 1.0987117299631253\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Thats a great loss value! ","metadata":{}},{"cell_type":"markdown","source":"# **Calculating Accuracy**","metadata":{}},{"cell_type":"markdown","source":"Doing that is very simple.\n\nWe basically run an **np.argmax()** function (function that returns the maximum \nalong a specified axis), **on our softmax outputs**, and then **find the mean of that being compared to our class targets**:","metadata":{}},{"cell_type":"code","source":"predictions = np.argmax(softmax_output_eg2, axis=1)\naccuracy = np.mean(predictions == class_targets_eg2)\n\nprint(f'acc: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T00:17:08.846836Z","iopub.execute_input":"2023-12-17T00:17:08.847348Z","iopub.status.idle":"2023-12-17T00:17:08.855591Z","shell.execute_reply.started":"2023-12-17T00:17:08.847309Z","shell.execute_reply":"2023-12-17T00:17:08.854281Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"acc: 1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Remember:** While **accuracy is practical and useful**, the **metric we most interested** in **while training a neural network is the loss metric**, which basically **tells how wrong something is**. Our goal is to **decrease that loss**.","metadata":{}},{"cell_type":"markdown","source":"Link to github repo: https://github.com/PrathamGhoshRoy/NeuralNetworkBasics/tree/main","metadata":{}}]}
