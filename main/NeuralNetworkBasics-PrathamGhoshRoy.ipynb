{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-04T00:25:34.792924Z","iopub.execute_input":"2023-12-04T00:25:34.793322Z","iopub.status.idle":"2023-12-04T00:25:34.827461Z","shell.execute_reply.started":"2023-12-04T00:25:34.793290Z","shell.execute_reply":"2023-12-04T00:25:34.826536Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network Basics\n\nThis is my personal documentation of my journey of learning the basics of Neural Networks.\n\nThanks to: [sentdex](https://www.youtube.com/@sentdex) for his lessons in NNFS (Neural Networks from Scratch)","metadata":{}},{"cell_type":"markdown","source":"#### First, lets import his module for accessing his datasets","metadata":{}},{"cell_type":"code","source":"!pip install nnfs","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:34.829169Z","iopub.execute_input":"2023-12-04T00:25:34.830179Z","iopub.status.idle":"2023-12-04T00:25:51.395414Z","shell.execute_reply.started":"2023-12-04T00:25:34.830122Z","shell.execute_reply":"2023-12-04T00:25:51.393974Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting nnfs\n  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nnfs) (1.24.3)\nInstalling collected packages: nnfs\nSuccessfully installed nnfs-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import nnfs","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.398394Z","iopub.execute_input":"2023-12-04T00:25:51.398857Z","iopub.status.idle":"2023-12-04T00:25:51.409540Z","shell.execute_reply.started":"2023-12-04T00:25:51.398822Z","shell.execute_reply":"2023-12-04T00:25:51.408264Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"An example of a 2-D array of inputs:","metadata":{}},{"cell_type":"code","source":"X = [[1, 2, 3, 2.5],\n     [2.0, 5.0, -1.0, 2.0],\n     [-1.5, 2.7, 3.3, -0,8]]","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.412560Z","iopub.execute_input":"2023-12-04T00:25:51.413101Z","iopub.status.idle":"2023-12-04T00:25:51.421476Z","shell.execute_reply.started":"2023-12-04T00:25:51.413065Z","shell.execute_reply":"2023-12-04T00:25:51.420622Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network:\nOkay, so here is a picture of a Neural Network for context:\n\n![](https://victorzhou.com/media/nn-series/network.svg)","metadata":{}},{"cell_type":"markdown","source":"It consists of:\n\n* **Neurons:** Those circles are called neurons.\n    It consists of:\n    * **Baises:** Values added (+) to the output.\n    * **Activation Function:** A function basically deciphers a neuron's importance in making the output decision.\n    \n        \n* **Layers:**\n    * Input Layer: The first layer where the model recieves its inputs (in blue)\n    * Hidden Layers: The middle two(or however many) consisting of 6 neurons(in this example) each (in black)\n    * Output Layer: The last layer at the end, which is the output of the model (in green)\n    \n    \n* **Weights:** The various lines which you see that joins each neuron of a particular layer to its neighbouring layers are called weights. They are multiplied with the previous layer's output.    \n","metadata":{}},{"cell_type":"markdown","source":"### The Neural Network Equation:\n\n\n![](https://pub.mdpi-res.com/universe/universe-08-00120/article_deploy/html/images/universe-08-00120-g001-550.jpg?1645603658)\n\n","metadata":{}},{"cell_type":"markdown","source":"Very similar to the equation: y = mx + b\nWhere,\n* **y** = output\n* **m** = weights\n* **x** = inputs\n* **b** = bias\n\nWhat it basically does is that it takes in the **inputs** from the **previous neuron**, **multiply them** with the **weights**, or the **connections(lines)**, and once it **reaches the current layer's neuron**, if and when it goes through the **activation function**, the **neuron's bias** gets added and then it **outputs** it to **the next layer**.","metadata":{}},{"cell_type":"markdown","source":"### Lets create our code for a Layer\n\n**Layer Dense**\n\nFirst, we are defining the weights:\n* **Weights** will be an array of **random numbers** of the dimension: **(number of inputs, n_neurons)** since **we need weights of each and every input per neuron.**\n\nWe will also **normalize** the weights by **multiplying it with a number(n) i.e. 1>n>0**, here lets take 0.10\n> self.weight = 0.10 * np.random.rand(n_inputs, n_neurons)\n\nNext, we will define the biases:\n\n* **Biases** will be (for now), **an array of zeroes** for **each neuron**, so the **dimension** will be: **(1, number of neurons)**\n\nWhy? \nWell, if the number of the biases are too big, it **might just explode** by the **time it comes to the output layer**. We **don't** want very big numbers because **they might explode and become incomputable**, hence we will keep the biases 0 for now.\n> self.biases = np.zeroes((1, n_neurons))\n\n**Drawback of having 0 biases:**\n\nIf **incase** the **sum of an equation for a neuron becomes 0** because of our bias (i.e. the value being added in the equation) is zero, the **input for the next layer will be 0**, and so will be its output, since anything multiplied by a 0 is 0. \n\n**We do not want** that **since that will create a ripple effect of outputs of 0 for each layer** and we will be left with something called a **dead network**.\n\nSo, if we have a **dead network**, try changing the biases to a **non-zero number**.","metadata":{}},{"cell_type":"markdown","source":"##### **Forward method of Layer Dense:**\n\nIt will simply output the dotproduct i.e. (y = mx) + b for each neuron, hence:\n> self.output = np.dot(inputs, self.weight) + self.biases","metadata":{}},{"cell_type":"code","source":"class Layer_Dense:\n    def __init__(self, n_inputs, n_neurons):\n        self.weight = 0.10 * np.random.rand(n_inputs, n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n        \n    def forward(self, inputs):\n        self.output = np.dot(inputs, self.weight) + self.biases      ","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.423327Z","iopub.execute_input":"2023-12-04T00:25:51.423745Z","iopub.status.idle":"2023-12-04T00:25:51.434433Z","shell.execute_reply.started":"2023-12-04T00:25:51.423712Z","shell.execute_reply":"2023-12-04T00:25:51.433100Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Activation Function\n\nEach neuron **should** consist of an activation function.\n\n**Activation Function:** A function which basically **decides whether** a **neuron should be activated or not** based on its **weighted sum of inputs**. The **choice of activation function** affects the network's ability to learn **complex patters** and **relationships** in data.\n\nFor our example, we will be using **ReLU (Rectified Linear Unit)** Function.","metadata":{}},{"cell_type":"markdown","source":"### Rectified Linear Unit (ReLU) function\n\n![](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24d1ac2cc1ded69730feb_relu.jpg)","metadata":{}},{"cell_type":"markdown","source":"**ReLU function:**\n\n> f(x) = max(0, x)\n\nIt basically **assings a value** of **0 (unactivated)** or **non-zero 'x' (activated)** to the **weighted sum of a neuron's inputs**.","metadata":{}},{"cell_type":"code","source":"class Activation_ReLU:\n    def forward(self, inputs):\n        self.output = np.maximum(0, inputs)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.436158Z","iopub.execute_input":"2023-12-04T00:25:51.436493Z","iopub.status.idle":"2023-12-04T00:25:51.445134Z","shell.execute_reply.started":"2023-12-04T00:25:51.436463Z","shell.execute_reply":"2023-12-04T00:25:51.444148Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Next we will make an **Softmax Activation** for our output layer","metadata":{}},{"cell_type":"markdown","source":"## Softmax Activation\n\n![](https://miro.medium.com/v2/resize:fit:1232/0*GxuMPOpGsMoN5RwI)","metadata":{}},{"cell_type":"markdown","source":"Softmax Activation function: \n\n![](https://docs-assets.developer.apple.com/published/c2185dfdcf/0ab139bc-3ff6-49d2-8b36-dcc98ef31102.png)","metadata":{}},{"cell_type":"markdown","source":"## Softmax vs Sigmoid\n\nSoftmax curve might be looking like the sigmoid curve, but there is quite a bit of difference: \n\n![](https://i.stack.imgur.com/iJ6vX.png)","metadata":{}},{"cell_type":"markdown","source":"## What is the softmax function doing?\n\nBasically, it is putting the output values (output of the model) through:\n* The exponential function i.e. -> y =e^x\n* Then normalizing it","metadata":{}},{"cell_type":"markdown","source":"**Remember:** The **inputs** here are the **outputs of our model/output layer**, hence it will be in **batch form** (since we will have a whole dataset of inputs for any given model, in most cases).\n\n","metadata":{}},{"cell_type":"markdown","source":"* First we will exponentiate each value subtracted by the max value in that batch.\n\n> y = e^(x - max(x))\n\nSo:\n\n> exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)\n\n#### Why are we doing that?\n\n\nWhat subtracting each value with that array's maximum value will basically do is:\n* Turn the **highest number** in that array into a **0**\n* **Rest of the numbers** will be **less than 0**\n\nThen when we exponentiate it:\n* The **max number, previously 0**, will **become 1** since **e^0 = 1.0**\n* All the **other numbers** in the array will be a **number less than 1** but **more than 0** (classification heaven: 0-1)\n* All other numbers(n) in the array will be: **1>n>0**","metadata":{}},{"cell_type":"markdown","source":"In the following code: \n> exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)\n\n**Remember, since:**\n\n* **we want the max of each respective array**, and **not the entire array**, we **do axis=1**.\n* And **keepdims=True** to **output the values** in the **same dimensions as the input**.","metadata":{}},{"cell_type":"markdown","source":"### Next, in the softmax function:\n\n* **Dividing the exponential values** by the **sum of the exponential values of respective arrays**\n\n**Why?**\n\nSo that we have a **normalized set of values for our outputs**, which will **help us achieve** our **goal of creating a neural network** which **works well with a multiple-classification problem**.\n","metadata":{}},{"cell_type":"code","source":"class Activation_Softmax:\n    def forward(self, inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.447065Z","iopub.execute_input":"2023-12-04T00:25:51.447446Z","iopub.status.idle":"2023-12-04T00:25:51.456790Z","shell.execute_reply.started":"2023-12-04T00:25:51.447416Z","shell.execute_reply":"2023-12-04T00:25:51.455667Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Now, will **input a dataset** which basically is **visually spiral** when **scatter plotted**.\n\nSomething like:\n\n![](https://telesens.co/wp-content/uploads/2017/09/img_59cbd07be1178.png)\n\nLink to spiral dataset code: https://github.com/Sentdex/nnfs/blob/master/nnfs/datasets/spiral.py","metadata":{}},{"cell_type":"markdown","source":"### Training Spiral Data:\n\nLets create training data in a **spiral format!**","metadata":{}},{"cell_type":"code","source":"from nnfs.datasets import spiral_data\n\nX, y = spiral_data(samples=100, classes=3)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.458051Z","iopub.execute_input":"2023-12-04T00:25:51.458367Z","iopub.status.idle":"2023-12-04T00:25:51.473205Z","shell.execute_reply.started":"2023-12-04T00:25:51.458338Z","shell.execute_reply":"2023-12-04T00:25:51.471872Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### For starters, lets create a **basic input layer**:","metadata":{}},{"cell_type":"code","source":"basic_dense1 = Layer_Dense(2,3)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.474760Z","iopub.execute_input":"2023-12-04T00:25:51.475376Z","iopub.status.idle":"2023-12-04T00:25:51.484012Z","shell.execute_reply.started":"2023-12-04T00:25:51.475345Z","shell.execute_reply":"2023-12-04T00:25:51.482940Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(basic_dense1)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.487155Z","iopub.execute_input":"2023-12-04T00:25:51.487464Z","iopub.status.idle":"2023-12-04T00:25:51.496392Z","shell.execute_reply.started":"2023-12-04T00:25:51.487436Z","shell.execute_reply":"2023-12-04T00:25:51.495241Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<__main__.Layer_Dense object at 0x79de06cd15a0>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Now, we create an **activation function** for basic_dense1","metadata":{}},{"cell_type":"code","source":"basic_activation1 = Activation_ReLU()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.497380Z","iopub.execute_input":"2023-12-04T00:25:51.497744Z","iopub.status.idle":"2023-12-04T00:25:51.507253Z","shell.execute_reply.started":"2023-12-04T00:25:51.497713Z","shell.execute_reply":"2023-12-04T00:25:51.506082Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### Now, lets create the output layer:\n\nWith **softmax activation**.","metadata":{}},{"cell_type":"code","source":"basic_dense2 = Layer_Dense(3, 3)\nbasic_activation2 = Activation_Softmax()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.508463Z","iopub.execute_input":"2023-12-04T00:25:51.509188Z","iopub.status.idle":"2023-12-04T00:25:51.520095Z","shell.execute_reply.started":"2023-12-04T00:25:51.509156Z","shell.execute_reply":"2023-12-04T00:25:51.519026Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Lets test it out!","metadata":{}},{"cell_type":"code","source":"basic_dense1.forward(X)\nbasic_activation1.forward(basic_dense1.output)\n\nbasic_dense2.forward(basic_activation1.output)\nbasic_activation2.forward(basic_dense2.output)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.521425Z","iopub.execute_input":"2023-12-04T00:25:51.521752Z","iopub.status.idle":"2023-12-04T00:25:51.536942Z","shell.execute_reply.started":"2023-12-04T00:25:51.521723Z","shell.execute_reply":"2023-12-04T00:25:51.536121Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(basic_activation2.output[:5])","metadata":{"execution":{"iopub.status.busy":"2023-12-04T00:25:51.537983Z","iopub.execute_input":"2023-12-04T00:25:51.538275Z","iopub.status.idle":"2023-12-04T00:25:51.545841Z","shell.execute_reply.started":"2023-12-04T00:25:51.538248Z","shell.execute_reply":"2023-12-04T00:25:51.544683Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[[0.33333333 0.33333333 0.33333333]\n [0.33333844 0.33332724 0.33333432]\n [0.33335953 0.33330719 0.33333328]\n [0.33337674 0.33329111 0.33333215]\n [0.3333764  0.33328889 0.33333471]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Everything is working great!\n\nThat was our first ever neural network! Congrats!\n\n3.12.2023","metadata":{}},{"cell_type":"markdown","source":"Link to github repo: https://github.com/PrathamGhoshRoy/NeuralNetworkBasics/tree/main","metadata":{}}]}
